{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b17c7a36-f8d3-4647-b380-fc24cddc0616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded lazy data:  dask.array<from-zarr, shape=(1, 1, 464, 1280, 928), dtype=uint16, chunksize=(1, 1, 128, 128, 128), chunktype=numpy.ndarray>\n",
      "Estimating super chunksize. Provided super chunksize: None - Target MB: 2048\n",
      "Estimated chunksize to fit in memory 2048 MiB: (464, 1280, 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading path from /data/best_model_097_2d.ckpt\n",
      "Total batches:  116.0 (464, 1280, 928)\n"
     ]
    }
   ],
   "source": [
    "from aind_large_scale_prediction.generator.utils import (\n",
    "    concatenate_lazy_data, recover_global_position, unpad_global_coords)\n",
    "from aind_large_scale_prediction.io import ImageReaderFactory\n",
    "from aind_large_scale_prediction.generator.dataset import create_data_loader\n",
    "import numpy as np\n",
    "import logging\n",
    "from aind_brain_segmentation.model.network import Neuratt\n",
    "import multiprocessing\n",
    "\n",
    "target_size_mb = 2048\n",
    "n_workers = 0\n",
    "super_chunksize = None#(200, 1024, 928)\n",
    "scale = 3\n",
    "smartspim_id = \"782646\"\n",
    "image_path = \"s3://aind-open-data/SmartSPIM_782646_2025-02-21_16-55-46_stitched_2025-03-06_08-22-36/image_tile_fusing/OMEZarr/Ex_639_Em_667.zarr\"\n",
    "#\"s3://aind-open-data/SmartSPIM_780343_2025-02-18_12-34-40_stitched_2025-02-25_08-15-10/image_tile_fusing/OMEZarr/Ex_639_Em_667.zarr\"\n",
    "# \"s3://aind-open-data/SmartSPIM_782746_2025-02-10_15-15-28_stitched_2025-02-25_08-50-23/image_tile_fusing/OMEZarr/Ex_639_Em_667.zarr\"\n",
    "# \"s3://aind-open-data/SmartSPIM_782744_2025-02-10_10-02-32_stitched_2025-02-25_09-53-12/image_tile_fusing/OMEZarr/Ex_639_Em_667.zarr\"\n",
    "# \"s3://aind-open-data/SmartSPIM_780343_2025-02-18_12-34-40_stitched_2025-02-25_08-15-10/image_tile_fusing/OMEZarr/Ex_639_Em_667.zarr\"\n",
    "#'s3://aind-open-data/SmartSPIM_729674_2024-12-16_12-16-28_stitched_2024-12-18_07-35-30/image_tile_fusing/OMEZarr/Ex_639_Em_680.zarr'\n",
    "#'s3://aind-open-data/SmartSPIM_761339_2025-01-10_21-19-15_stitched_2025-01-12_05-39-33/image_tile_fusing/OMEZarr/Ex_488_Em_525.zarr'\n",
    "checkpoint_path = \"/data/best_model_097_2d.ckpt\"\n",
    "#\"/results/whole_brain_seg/whole_brain_seg/cymgslil/checkpoints/best_model.ckpt\"\n",
    "\n",
    "# \"/scratch/latest_best_model_smartspim_clamp_int.ckpt\"\n",
    "#\"/data/smartspim_brain_seg_models/whole_brain_seg/whole_brain_seg/cfelpja3/checkpoints/best_model.ckpt\"\n",
    "\n",
    "device = None\n",
    "\n",
    "pin_memory = True\n",
    "if device is not None:\n",
    "    pin_memory = False\n",
    "    multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "    \n",
    "axis_pad = 0\n",
    "overlap_prediction_chunksize = (axis_pad, axis_pad, axis_pad)\n",
    "\n",
    "lazy_data = (\n",
    "    ImageReaderFactory()\n",
    "    .create(data_path=str(image_path), parse_path=False, multiscale=scale)\n",
    "    .as_dask_array()\n",
    ")\n",
    "\n",
    "prediction_chunksize = (4, lazy_data.shape[-2], lazy_data.shape[-1])\n",
    "\n",
    "logger = logging.Logger(name=\"log\")\n",
    "\n",
    "print(\"Loaded lazy data: \", lazy_data)\n",
    "batch_size = 1\n",
    "dtype = np.float32\n",
    "zarr_data_loader, zarr_dataset = create_data_loader(\n",
    "    lazy_data=lazy_data,\n",
    "    target_size_mb=target_size_mb,\n",
    "    prediction_chunksize=prediction_chunksize,\n",
    "    overlap_prediction_chunksize=overlap_prediction_chunksize,\n",
    "    n_workers=n_workers,\n",
    "    batch_size=batch_size,\n",
    "    dtype=dtype,  # Allowed data type to process with pytorch cuda\n",
    "    super_chunksize=super_chunksize,\n",
    "    lazy_callback_fn=None,  # partial_lazy_deskewing,\n",
    "    logger=logger,\n",
    "    device=device,\n",
    "    pin_memory=pin_memory,\n",
    "    override_suggested_cpus=False,\n",
    "    drop_last=True,\n",
    "    locked_array=False,\n",
    ")\n",
    "\n",
    "# Creating model\n",
    "segmentation_model = Neuratt()\n",
    "\n",
    "if checkpoint_path:\n",
    "    print(f\"Loading path from {checkpoint_path}\")\n",
    "    segmentation_model = Neuratt.load_from_checkpoint(checkpoint_path)\n",
    "\n",
    "total_batches = sum(zarr_dataset.internal_slice_sum) / batch_size\n",
    "print(\"Total batches: \", total_batches, zarr_dataset.lazy_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "294a0fa7-5fd2-4f23-87bf-0c51dc6efc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 1.03 GiB </td>\n",
       "                        <td> 4.00 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (1, 1, 464, 1280, 928) </td>\n",
       "                        <td> (1, 1, 128, 128, 128) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 320 chunks in 2 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> uint16 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"352\" height=\"195\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"25\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,25.412616514582485 0.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >1</text>\n",
       "  <text x=\"45.412617\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,45.412617,12.706308)\">1</text>\n",
       "\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"95\" y1=\"12\" x2=\"120\" y2=\"37\" />\n",
       "  <line x1=\"95\" y1=\"24\" x2=\"120\" y2=\"49\" />\n",
       "  <line x1=\"95\" y1=\"36\" x2=\"120\" y2=\"61\" />\n",
       "  <line x1=\"95\" y1=\"48\" x2=\"120\" y2=\"73\" />\n",
       "  <line x1=\"95\" y1=\"60\" x2=\"120\" y2=\"85\" />\n",
       "  <line x1=\"95\" y1=\"72\" x2=\"120\" y2=\"97\" />\n",
       "  <line x1=\"95\" y1=\"84\" x2=\"120\" y2=\"109\" />\n",
       "  <line x1=\"95\" y1=\"96\" x2=\"120\" y2=\"121\" />\n",
       "  <line x1=\"95\" y1=\"108\" x2=\"120\" y2=\"133\" />\n",
       "  <line x1=\"95\" y1=\"120\" x2=\"120\" y2=\"145\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"95\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"102\" y1=\"7\" x2=\"102\" y2=\"127\" />\n",
       "  <line x1=\"109\" y1=\"14\" x2=\"109\" y2=\"134\" />\n",
       "  <line x1=\"116\" y1=\"21\" x2=\"116\" y2=\"141\" />\n",
       "  <line x1=\"120\" y1=\"25\" x2=\"120\" y2=\"145\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"95.0,0.0 120.96521372150383,25.965213721503837 120.96521372150383,145.96521372150383 95.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"182\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"102\" y1=\"7\" x2=\"189\" y2=\"7\" />\n",
       "  <line x1=\"109\" y1=\"14\" x2=\"196\" y2=\"14\" />\n",
       "  <line x1=\"116\" y1=\"21\" x2=\"203\" y2=\"21\" />\n",
       "  <line x1=\"120\" y1=\"25\" x2=\"207\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"107\" y1=\"0\" x2=\"132\" y2=\"25\" />\n",
       "  <line x1=\"119\" y1=\"0\" x2=\"144\" y2=\"25\" />\n",
       "  <line x1=\"131\" y1=\"0\" x2=\"156\" y2=\"25\" />\n",
       "  <line x1=\"143\" y1=\"0\" x2=\"168\" y2=\"25\" />\n",
       "  <line x1=\"155\" y1=\"0\" x2=\"180\" y2=\"25\" />\n",
       "  <line x1=\"167\" y1=\"0\" x2=\"192\" y2=\"25\" />\n",
       "  <line x1=\"179\" y1=\"0\" x2=\"204\" y2=\"25\" />\n",
       "  <line x1=\"182\" y1=\"0\" x2=\"207\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"95.0,0.0 182.0,0.0 207.96521372150383,25.965213721503837 120.96521372150383,25.965213721503837\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"120\" y1=\"25\" x2=\"207\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"120\" y1=\"37\" x2=\"207\" y2=\"37\" />\n",
       "  <line x1=\"120\" y1=\"49\" x2=\"207\" y2=\"49\" />\n",
       "  <line x1=\"120\" y1=\"61\" x2=\"207\" y2=\"61\" />\n",
       "  <line x1=\"120\" y1=\"73\" x2=\"207\" y2=\"73\" />\n",
       "  <line x1=\"120\" y1=\"85\" x2=\"207\" y2=\"85\" />\n",
       "  <line x1=\"120\" y1=\"97\" x2=\"207\" y2=\"97\" />\n",
       "  <line x1=\"120\" y1=\"109\" x2=\"207\" y2=\"109\" />\n",
       "  <line x1=\"120\" y1=\"121\" x2=\"207\" y2=\"121\" />\n",
       "  <line x1=\"120\" y1=\"133\" x2=\"207\" y2=\"133\" />\n",
       "  <line x1=\"120\" y1=\"145\" x2=\"207\" y2=\"145\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"120\" y1=\"25\" x2=\"120\" y2=\"145\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"132\" y1=\"25\" x2=\"132\" y2=\"145\" />\n",
       "  <line x1=\"144\" y1=\"25\" x2=\"144\" y2=\"145\" />\n",
       "  <line x1=\"156\" y1=\"25\" x2=\"156\" y2=\"145\" />\n",
       "  <line x1=\"168\" y1=\"25\" x2=\"168\" y2=\"145\" />\n",
       "  <line x1=\"180\" y1=\"25\" x2=\"180\" y2=\"145\" />\n",
       "  <line x1=\"192\" y1=\"25\" x2=\"192\" y2=\"145\" />\n",
       "  <line x1=\"204\" y1=\"25\" x2=\"204\" y2=\"145\" />\n",
       "  <line x1=\"207\" y1=\"25\" x2=\"207\" y2=\"145\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"120.96521372150383,25.965213721503837 207.96521372150383,25.965213721503837 207.96521372150383,145.96521372150383 120.96521372150383,145.96521372150383\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"164.465214\" y=\"165.965214\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >928</text>\n",
       "  <text x=\"227.965214\" y=\"85.965214\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,227.965214,85.965214)\">1280</text>\n",
       "  <text x=\"97.982607\" y=\"152.982607\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,97.982607,152.982607)\">464</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<from-zarr, shape=(1, 1, 464, 1280, 928), dtype=uint16, chunksize=(1, 1, 128, 128, 128), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lazy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac529b57-5194-47a1-a100-2c922b26f657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 2.05 GiB </td>\n",
       "                        <td> 8.00 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (464, 1280, 928) </td>\n",
       "                        <td> (128, 128, 128) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 320 chunks in 4 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float32 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"172\" height=\"195\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"35\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"12\" x2=\"35\" y2=\"37\" />\n",
       "  <line x1=\"10\" y1=\"24\" x2=\"35\" y2=\"49\" />\n",
       "  <line x1=\"10\" y1=\"36\" x2=\"35\" y2=\"61\" />\n",
       "  <line x1=\"10\" y1=\"48\" x2=\"35\" y2=\"73\" />\n",
       "  <line x1=\"10\" y1=\"60\" x2=\"35\" y2=\"85\" />\n",
       "  <line x1=\"10\" y1=\"72\" x2=\"35\" y2=\"97\" />\n",
       "  <line x1=\"10\" y1=\"84\" x2=\"35\" y2=\"109\" />\n",
       "  <line x1=\"10\" y1=\"96\" x2=\"35\" y2=\"121\" />\n",
       "  <line x1=\"10\" y1=\"108\" x2=\"35\" y2=\"133\" />\n",
       "  <line x1=\"10\" y1=\"120\" x2=\"35\" y2=\"145\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"17\" y1=\"7\" x2=\"17\" y2=\"127\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"24\" y2=\"134\" />\n",
       "  <line x1=\"31\" y1=\"21\" x2=\"31\" y2=\"141\" />\n",
       "  <line x1=\"35\" y1=\"25\" x2=\"35\" y2=\"145\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 35.96521372150384,25.965213721503837 35.96521372150384,145.96521372150383 10.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"97\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"17\" y1=\"7\" x2=\"104\" y2=\"7\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"111\" y2=\"14\" />\n",
       "  <line x1=\"31\" y1=\"21\" x2=\"118\" y2=\"21\" />\n",
       "  <line x1=\"35\" y1=\"25\" x2=\"122\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"35\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"22\" y1=\"0\" x2=\"47\" y2=\"25\" />\n",
       "  <line x1=\"34\" y1=\"0\" x2=\"59\" y2=\"25\" />\n",
       "  <line x1=\"46\" y1=\"0\" x2=\"71\" y2=\"25\" />\n",
       "  <line x1=\"58\" y1=\"0\" x2=\"83\" y2=\"25\" />\n",
       "  <line x1=\"70\" y1=\"0\" x2=\"95\" y2=\"25\" />\n",
       "  <line x1=\"82\" y1=\"0\" x2=\"107\" y2=\"25\" />\n",
       "  <line x1=\"94\" y1=\"0\" x2=\"119\" y2=\"25\" />\n",
       "  <line x1=\"97\" y1=\"0\" x2=\"122\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 97.0,0.0 122.96521372150383,25.965213721503837 35.96521372150384,25.965213721503837\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"35\" y1=\"25\" x2=\"122\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"35\" y1=\"37\" x2=\"122\" y2=\"37\" />\n",
       "  <line x1=\"35\" y1=\"49\" x2=\"122\" y2=\"49\" />\n",
       "  <line x1=\"35\" y1=\"61\" x2=\"122\" y2=\"61\" />\n",
       "  <line x1=\"35\" y1=\"73\" x2=\"122\" y2=\"73\" />\n",
       "  <line x1=\"35\" y1=\"85\" x2=\"122\" y2=\"85\" />\n",
       "  <line x1=\"35\" y1=\"97\" x2=\"122\" y2=\"97\" />\n",
       "  <line x1=\"35\" y1=\"109\" x2=\"122\" y2=\"109\" />\n",
       "  <line x1=\"35\" y1=\"121\" x2=\"122\" y2=\"121\" />\n",
       "  <line x1=\"35\" y1=\"133\" x2=\"122\" y2=\"133\" />\n",
       "  <line x1=\"35\" y1=\"145\" x2=\"122\" y2=\"145\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"35\" y1=\"25\" x2=\"35\" y2=\"145\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"47\" y1=\"25\" x2=\"47\" y2=\"145\" />\n",
       "  <line x1=\"59\" y1=\"25\" x2=\"59\" y2=\"145\" />\n",
       "  <line x1=\"71\" y1=\"25\" x2=\"71\" y2=\"145\" />\n",
       "  <line x1=\"83\" y1=\"25\" x2=\"83\" y2=\"145\" />\n",
       "  <line x1=\"95\" y1=\"25\" x2=\"95\" y2=\"145\" />\n",
       "  <line x1=\"107\" y1=\"25\" x2=\"107\" y2=\"145\" />\n",
       "  <line x1=\"119\" y1=\"25\" x2=\"119\" y2=\"145\" />\n",
       "  <line x1=\"122\" y1=\"25\" x2=\"122\" y2=\"145\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"35.96521372150384,25.965213721503837 122.96521372150383,25.965213721503837 122.96521372150383,145.96521372150383 35.96521372150384,145.96521372150383\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"79.465214\" y=\"165.965214\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >928</text>\n",
       "  <text x=\"142.965214\" y=\"85.965214\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,142.965214,85.965214)\">1280</text>\n",
       "  <text x=\"12.982607\" y=\"152.982607\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,12.982607,152.982607)\">464</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<astype, shape=(464, 1280, 928), dtype=float32, chunksize=(128, 128, 128), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zarr_dataset.lazy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b976be7-e815-46bd-91d9-4022574238ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_indices = zarr_dataset.lazy_data.shape[-3:] // np.array(prediction_chunksize)\n",
    "# output_shape = tuple(\n",
    "#     n * c - (n - 1) * o\n",
    "#     for n, c, o in zip(chunk_indices, prediction_chunksize, overlap_prediction_chunksize)\n",
    "# )\n",
    "\n",
    "# chunk_indices_grid = np.array(\n",
    "#     np.meshgrid(\n",
    "#         np.arange(chunk_indices[0]),\n",
    "#         np.arange(chunk_indices[1]),\n",
    "#         np.arange(chunk_indices[2]),\n",
    "#         indexing='ij'\n",
    "#     )\n",
    "# ).reshape(3, -1).T\n",
    "\n",
    "# print(chunk_indices, output_shape, chunk_indices_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10c59703-a121-4ad1-ac99-4a8ba7b3262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_start = np.array([ix, iy, iz]) * (np.array(chunk_size) - np.array(overlap_size))\n",
    "# global_end = global_start + valid_end - valid_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "805544b1-e9da-474d-906b-2589696d9e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 464, 1280, 928) (1, 1, 464, 1280, 928) (1, 1, 464, 1280, 928)\n"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "\n",
    "output_seg_path = f\"/scratch/intermediate_seg_{smartspim_id}.zarr\"\n",
    "output_prob_path = f\"/scratch/intermediate_prob_{smartspim_id}.zarr\"\n",
    "output_data_path = f\"/scratch/data_{smartspim_id}.zarr\"\n",
    "\n",
    "output_intermediate_seg = zarr.open(\n",
    "    output_seg_path,\n",
    "    \"w\",\n",
    "    shape=(\n",
    "        1,\n",
    "        1,\n",
    "    )\n",
    "    + zarr_dataset.lazy_data.shape[-3:],\n",
    "    chunks=(\n",
    "        1,\n",
    "        1,\n",
    "    )\n",
    "    + (128, 128, 128),\n",
    "    dtype=np.uint8,\n",
    ")\n",
    "\n",
    "output_intermediate_prob = zarr.open(\n",
    "    output_prob_path,\n",
    "    \"w\",\n",
    "    shape=(\n",
    "        1,\n",
    "        1,\n",
    "    )\n",
    "    + zarr_dataset.lazy_data.shape[-3:],\n",
    "    chunks=(\n",
    "        1,\n",
    "        1,\n",
    "    )\n",
    "    + (128, 128, 128),\n",
    "    dtype=np.float16,\n",
    ")\n",
    "\n",
    "output_raw_data = zarr.open(\n",
    "    output_data_path,\n",
    "    \"w\",\n",
    "    shape=(\n",
    "        1,\n",
    "        1,\n",
    "    )\n",
    "    + zarr_dataset.lazy_data.shape[-3:],\n",
    "    chunks=(\n",
    "        1,\n",
    "        1,\n",
    "    )\n",
    "    + (128, 128, 128),\n",
    "    dtype=np.uint16,\n",
    ")\n",
    "\n",
    "shape = zarr_dataset.lazy_data.shape[-3:]\n",
    "print(output_intermediate_seg.shape, output_intermediate_prob.shape, output_raw_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba146264-2e22-4232-9e43-babb69380df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neuratt(\n",
       "  (model): SwinUNETR(\n",
       "    (swinViT): SwinTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(1, 12, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers1): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=12, out_features=36, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=12, out_features=48, bias=True)\n",
       "                (linear2): Linear(in_features=48, out_features=12, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=48, out_features=24, bias=False)\n",
       "            (norm): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers2): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=24, out_features=72, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=24, out_features=96, bias=True)\n",
       "                (linear2): Linear(in_features=96, out_features=24, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=96, out_features=48, bias=False)\n",
       "            (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers3): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=48, out_features=192, bias=True)\n",
       "                (linear2): Linear(in_features=192, out_features=48, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=192, out_features=96, bias=False)\n",
       "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers4): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (linear2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (encoder1): UnetrBasicBlock(\n",
       "      (layer): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (conv3): Convolution(\n",
       "          (conv): Conv2d(1, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (norm3): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (encoder2): UnetrBasicBlock(\n",
       "      (layer): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (encoder3): UnetrBasicBlock(\n",
       "      (layer): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (encoder4): UnetrBasicBlock(\n",
       "      (layer): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (encoder10): UnetrBasicBlock(\n",
       "      (layer): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (decoder5): UnetrUpBlock(\n",
       "      (transp_conv): Convolution(\n",
       "        (conv): ConvTranspose2d(192, 96, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "      )\n",
       "      (conv_block): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (conv3): Convolution(\n",
       "          (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (norm3): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (decoder4): UnetrUpBlock(\n",
       "      (transp_conv): Convolution(\n",
       "        (conv): ConvTranspose2d(96, 48, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "      )\n",
       "      (conv_block): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (conv3): Convolution(\n",
       "          (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (norm3): InstanceNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (decoder3): UnetrUpBlock(\n",
       "      (transp_conv): Convolution(\n",
       "        (conv): ConvTranspose2d(48, 24, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "      )\n",
       "      (conv_block): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (conv3): Convolution(\n",
       "          (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (norm3): InstanceNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (decoder2): UnetrUpBlock(\n",
       "      (transp_conv): Convolution(\n",
       "        (conv): ConvTranspose2d(24, 12, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "      )\n",
       "      (conv_block): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (conv3): Convolution(\n",
       "          (conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (norm3): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (decoder1): UnetrUpBlock(\n",
       "      (transp_conv): Convolution(\n",
       "        (conv): ConvTranspose2d(12, 12, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "      )\n",
       "      (conv_block): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (conv3): Convolution(\n",
       "          (conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (norm3): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (out): UnetOutBlock(\n",
       "      (conv): Convolution(\n",
       "        (conv): Conv2d(12, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_data): NormalizeAndRescaleWrapper(\n",
       "    (normalize): Normalize(p=1.0, p_batch=1.0, same_on_batch=True, mean=tensor([0.]), std=tensor([1.]))\n",
       "  )\n",
       "  (loss_fn): BCEWithLogitsLoss()\n",
       "  (dice_score_metric): BinaryF1Score()\n",
       "  (jaccard_index_metric): BinaryJaccardIndex()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmentation_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "234453d7-6297-4e21-9ebb-8bcae8148875",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2db92c47-a6b2-4b10-bd28-9086bb0aed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cuda_device = torch.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "148828b0-4686-4ddf-a40e-2d2d4a23328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import binary_fill_holes\n",
    "from scipy.ndimage import binary_closing\n",
    "from skimage.morphology import ball\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage.morphology import remove_small_objects\n",
    "\n",
    "def post_process_mask(mask, threshold=0.5, min_size=100):\n",
    "    mask = binary_fill_holes(mask)\n",
    "    mask = remove_small_objects(mask, min_size=min_size)\n",
    "    \n",
    "    # structuring_element = ball(radius=5)\n",
    "    mask = binary_closing(mask)#, structure=structuring_element)\n",
    "    \n",
    "    mask = gaussian_filter(mask.astype(float), sigma=1)\n",
    "\n",
    "    mask = mask > threshold\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77df998c-a18b-459d-ad39-dd18cd08f815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape:  (1280, 928)\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(0, 4, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(4, 8, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(8, 12, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(12, 16, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(16, 20, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(20, 24, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(24, 28, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(28, 32, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(32, 36, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(36, 40, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(40, 44, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(44, 48, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(48, 52, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(52, 56, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999998807907104\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(56, 60, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999998807907104\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999997615814209\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(60, 64, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999997615814209\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999994039535522\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(64, 68, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999994039535522\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999982118606567\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(68, 72, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999982118606567\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999984502792358\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(72, 76, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999984502792358\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.999997615814209\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(76, 80, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.999997615814209\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999957084655762\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(80, 84, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999957084655762\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999948740005493\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(84, 88, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999948740005493\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999938011169434\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(88, 92, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999938011169434\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999949932098389\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(92, 96, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999949932098389\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999961853027344\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(96, 100, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999961853027344\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.999994158744812\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(100, 104, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.999994158744812\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999938011169434\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(104, 108, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999938011169434\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.999995231628418\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(108, 112, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.999995231628418\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999949932098389\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(112, 116, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999949932098389\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999932050704956\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(116, 120, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999932050704956\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.999991774559021\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(120, 124, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.999991774559021\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999852180480957\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(124, 128, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999852180480957\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999827146530151\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(128, 132, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999827146530151\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999783039093018\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(132, 136, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999783039093018\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999841451644897\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(136, 140, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999841451644897\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999818801879883\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(140, 144, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999818801879883\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999806880950928\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(144, 148, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999806880950928\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999796152114868\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(148, 152, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999796152114868\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999808073043823\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(152, 156, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999808073043823\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.99998939037323\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(156, 160, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.99998939037323\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999823570251465\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(160, 164, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999823570251465\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999760389328003\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(164, 168, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999760389328003\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999724626541138\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(168, 172, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999724626541138\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999734163284302\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(172, 176, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999734163284302\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999696016311646\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(176, 180, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999696016311646\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999855756759644\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(180, 184, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999855756759644\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999774694442749\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(184, 188, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999774694442749\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999570846557617\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(188, 192, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999570846557617\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999552965164185\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(192, 196, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999552965164185\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999502897262573\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(196, 200, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999502897262573\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999436140060425\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(200, 204, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999436140060425\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999406337738037\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(204, 208, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999406337738037\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999434947967529\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(208, 212, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999434947967529\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.999950647354126\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(212, 216, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.999950647354126\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999287128448486\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(216, 220, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999287128448486\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999279975891113\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(220, 224, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999279975891113\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.999924898147583\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(224, 228, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.999924898147583\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999175071716309\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(228, 232, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999175071716309\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.999927282333374\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(232, 236, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.999927282333374\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.999934196472168\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(236, 240, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.999934196472168\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999436140060425\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(240, 244, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999436140060425\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999508857727051\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(244, 248, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999508857727051\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999544620513916\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(248, 252, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999544620513916\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999589920043945\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(252, 256, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999589920043945\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999617338180542\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(256, 260, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999617338180542\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999755620956421\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(260, 264, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999755620956421\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999721050262451\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(264, 268, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999721050262451\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999946355819702\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(268, 272, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999946355819702\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999788999557495\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(272, 276, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999788999557495\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.999987006187439\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(276, 280, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.999987006187439\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999842643737793\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(280, 284, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999842643737793\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999817609786987\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(284, 288, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999817609786987\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999837875366211\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(288, 292, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999837875366211\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999901056289673\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(292, 296, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999901056289673\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999874830245972\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(296, 300, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999874830245972\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999912977218628\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(300, 304, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999912977218628\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999854564666748\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(304, 308, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999854564666748\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999825954437256\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(308, 312, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999825954437256\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999878406524658\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(312, 316, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999878406524658\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999946355819702\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(316, 320, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999946355819702\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999895095825195\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(320, 324, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999895095825195\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999920129776001\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(324, 328, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999920129776001\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999908208847046\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(328, 332, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999908208847046\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.999993085861206\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(332, 336, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.999993085861206\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999953508377075\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(336, 340, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999953508377075\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999966621398926\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(340, 344, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999966621398926\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999967813491821\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(344, 348, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999967813491821\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999970197677612\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(348, 352, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999970197677612\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999985694885254\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(352, 356, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999985694885254\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999972581863403\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(356, 360, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999972581863403\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999985694885254\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(360, 364, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999985694885254\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 0.9999996423721313\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(364, 368, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 0.9999996423721313\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(368, 372, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(372, 376, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(376, 380, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(380, 384, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(384, 388, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(388, 392, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(392, 396, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(396, 400, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(400, 404, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(404, 408, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(408, 412, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(412, 416, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(416, 420, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(420, 424, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(424, 428, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(428, 432, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(432, 436, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(436, 440, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(440, 444, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(444, 448, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(448, 452, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n",
      "Slice orig: (4, 1280, 928)\n",
      "Slice data: torch.Size([4, 1, 1024, 1024])\n",
      "Block torch.Size([4, 1, 1024, 1024]) - Max mask: 1.0\n",
      "Tensor shape: torch.Size([1, 4, 1280, 928]) - Pred mask -> (4, 1024, 1024) - unpadded_global_slice: (slice(0, 1, None), slice(0, 1, None), slice(452, 456, None), slice(0, 1280, None), slice(0, 928, None)) Max mask: 1.0\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# import cupy as cp\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2\n",
    "from skimage.transform import resize as ski_resize\n",
    "\n",
    "image_height = 1024\n",
    "image_width = 1024\n",
    "orig_shape = zarr_dataset.lazy_data.shape[-2:]\n",
    "THRES = 0.7\n",
    "print(\"Original shape: \", orig_shape)\n",
    "\n",
    "for i, sample in enumerate(zarr_data_loader):\n",
    "    # Load batch as a NumPy array (but keep it on GPU when possible)\n",
    "    slice_data_orig = np.squeeze(sample.batch_tensor.numpy())\n",
    "\n",
    "    # Preallocate array in GPU memory for batch resizing\n",
    "    slice_data = torch.zeros(\n",
    "        (slice_data_orig.shape[0], 1, image_height, image_width), device=cuda_device, dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    print(\"Slice orig:\", slice_data_orig.shape)\n",
    "\n",
    "    # **GPU-Accelerated Resizing**\n",
    "    for i in range(slice_data_orig.shape[0]):\n",
    "        slice_img = torch.tensor(slice_data_orig[i], device=cuda_device).unsqueeze(0).unsqueeze(0)\n",
    "        slice_data[i] = F.interpolate(slice_img, size=(image_height, image_width), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    print(\"Slice data:\", slice_data.shape)\n",
    "\n",
    "    # **Model Prediction**\n",
    "    pred_mask, prob_mask = segmentation_model.predict(batch=slice_data, threshold=THRES)\n",
    "\n",
    "    print(f\"Block {slice_data.shape} - Max mask: {prob_mask.max()}\")\n",
    "\n",
    "    # Move to CPU and convert to NumPy in one step (avoiding unnecessary copies)\n",
    "    pred_mask = pred_mask.squeeze().detach().cpu().numpy()\n",
    "    prob_mask = prob_mask.squeeze().detach().cpu().numpy()\n",
    "\n",
    "    # **Preallocate Resized Masks on CPU**\n",
    "    pred_mask_resampled = np.zeros((pred_mask.shape[0],) + orig_shape, dtype=np.uint8)\n",
    "    prob_mask_resampled = np.zeros((prob_mask.shape[0],) + orig_shape, dtype=np.float32)\n",
    "\n",
    "    # **Resize with OpenCV (Faster than skimage)**\n",
    "    for i in range(pred_mask.shape[0]):\n",
    "        pred_mask_resampled[i] = cv2.resize(pred_mask[i], orig_shape[::-1], interpolation=cv2.INTER_NEAREST)\n",
    "        prob_mask_resampled[i] = cv2.resize(prob_mask[i], orig_shape[::-1], interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # **Compute Global Positions Efficiently**\n",
    "    (\n",
    "        global_coord_pos,\n",
    "        global_coord_positions_start,\n",
    "        global_coord_positions_end,\n",
    "    ) = recover_global_position(\n",
    "        super_chunk_slice=sample.batch_super_chunk[0],\n",
    "        internal_slices=sample.batch_internal_slice,\n",
    "    )\n",
    "\n",
    "    unpadded_global_slice, unpadded_local_slice = unpad_global_coords(\n",
    "        global_coord_pos=global_coord_pos[-3:],\n",
    "        block_shape=slice_data.shape[-3:],\n",
    "        overlap_prediction_chunksize=overlap_prediction_chunksize[-3:],\n",
    "        dataset_shape=zarr_dataset.lazy_data.shape[-3:],\n",
    "    )\n",
    "\n",
    "    # **Move Tensors Off GPU to Free Memory**\n",
    "    del slice_data\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    unpadded_global_slice = (slice(0, 1), slice(0, 1)) + unpadded_global_slice\n",
    "\n",
    "    print(\n",
    "        f\"Tensor shape: {sample.batch_tensor.shape} - Pred mask -> {pred_mask.shape} - unpadded_global_slice: {unpadded_global_slice} Max mask: {prob_mask.max()}\"\n",
    "    )\n",
    "    # **Store Output Efficiently**\n",
    "    output_intermediate_seg[unpadded_global_slice] = pred_mask_resampled[None, None, ...]#[unpadded_global_slice]\n",
    "    output_raw_data[unpadded_global_slice] = slice_data_orig[None, None, ...]#[unpadded_global_slice]\n",
    "    output_intermediate_prob[unpadded_global_slice] = prob_mask_resampled[None, None, ...]#[unpadded_global_slice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5e4ffdd-7a95-4364-9387-e64b0c8e133a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/intermediate_seg_782646.zarr\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 525.62 MiB </td>\n",
       "                        <td> 2.00 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (1, 1, 464, 1280, 928) </td>\n",
       "                        <td> (1, 1, 128, 128, 128) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 320 chunks in 2 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> uint8 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"352\" height=\"195\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"25\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,25.412616514582485 0.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >1</text>\n",
       "  <text x=\"45.412617\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,45.412617,12.706308)\">1</text>\n",
       "\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"95\" y1=\"12\" x2=\"120\" y2=\"37\" />\n",
       "  <line x1=\"95\" y1=\"24\" x2=\"120\" y2=\"49\" />\n",
       "  <line x1=\"95\" y1=\"36\" x2=\"120\" y2=\"61\" />\n",
       "  <line x1=\"95\" y1=\"48\" x2=\"120\" y2=\"73\" />\n",
       "  <line x1=\"95\" y1=\"60\" x2=\"120\" y2=\"85\" />\n",
       "  <line x1=\"95\" y1=\"72\" x2=\"120\" y2=\"97\" />\n",
       "  <line x1=\"95\" y1=\"84\" x2=\"120\" y2=\"109\" />\n",
       "  <line x1=\"95\" y1=\"96\" x2=\"120\" y2=\"121\" />\n",
       "  <line x1=\"95\" y1=\"108\" x2=\"120\" y2=\"133\" />\n",
       "  <line x1=\"95\" y1=\"120\" x2=\"120\" y2=\"145\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"95\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"102\" y1=\"7\" x2=\"102\" y2=\"127\" />\n",
       "  <line x1=\"109\" y1=\"14\" x2=\"109\" y2=\"134\" />\n",
       "  <line x1=\"116\" y1=\"21\" x2=\"116\" y2=\"141\" />\n",
       "  <line x1=\"120\" y1=\"25\" x2=\"120\" y2=\"145\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"95.0,0.0 120.96521372150383,25.965213721503837 120.96521372150383,145.96521372150383 95.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"182\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"102\" y1=\"7\" x2=\"189\" y2=\"7\" />\n",
       "  <line x1=\"109\" y1=\"14\" x2=\"196\" y2=\"14\" />\n",
       "  <line x1=\"116\" y1=\"21\" x2=\"203\" y2=\"21\" />\n",
       "  <line x1=\"120\" y1=\"25\" x2=\"207\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"107\" y1=\"0\" x2=\"132\" y2=\"25\" />\n",
       "  <line x1=\"119\" y1=\"0\" x2=\"144\" y2=\"25\" />\n",
       "  <line x1=\"131\" y1=\"0\" x2=\"156\" y2=\"25\" />\n",
       "  <line x1=\"143\" y1=\"0\" x2=\"168\" y2=\"25\" />\n",
       "  <line x1=\"155\" y1=\"0\" x2=\"180\" y2=\"25\" />\n",
       "  <line x1=\"167\" y1=\"0\" x2=\"192\" y2=\"25\" />\n",
       "  <line x1=\"179\" y1=\"0\" x2=\"204\" y2=\"25\" />\n",
       "  <line x1=\"182\" y1=\"0\" x2=\"207\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"95.0,0.0 182.0,0.0 207.96521372150383,25.965213721503837 120.96521372150383,25.965213721503837\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"120\" y1=\"25\" x2=\"207\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"120\" y1=\"37\" x2=\"207\" y2=\"37\" />\n",
       "  <line x1=\"120\" y1=\"49\" x2=\"207\" y2=\"49\" />\n",
       "  <line x1=\"120\" y1=\"61\" x2=\"207\" y2=\"61\" />\n",
       "  <line x1=\"120\" y1=\"73\" x2=\"207\" y2=\"73\" />\n",
       "  <line x1=\"120\" y1=\"85\" x2=\"207\" y2=\"85\" />\n",
       "  <line x1=\"120\" y1=\"97\" x2=\"207\" y2=\"97\" />\n",
       "  <line x1=\"120\" y1=\"109\" x2=\"207\" y2=\"109\" />\n",
       "  <line x1=\"120\" y1=\"121\" x2=\"207\" y2=\"121\" />\n",
       "  <line x1=\"120\" y1=\"133\" x2=\"207\" y2=\"133\" />\n",
       "  <line x1=\"120\" y1=\"145\" x2=\"207\" y2=\"145\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"120\" y1=\"25\" x2=\"120\" y2=\"145\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"132\" y1=\"25\" x2=\"132\" y2=\"145\" />\n",
       "  <line x1=\"144\" y1=\"25\" x2=\"144\" y2=\"145\" />\n",
       "  <line x1=\"156\" y1=\"25\" x2=\"156\" y2=\"145\" />\n",
       "  <line x1=\"168\" y1=\"25\" x2=\"168\" y2=\"145\" />\n",
       "  <line x1=\"180\" y1=\"25\" x2=\"180\" y2=\"145\" />\n",
       "  <line x1=\"192\" y1=\"25\" x2=\"192\" y2=\"145\" />\n",
       "  <line x1=\"204\" y1=\"25\" x2=\"204\" y2=\"145\" />\n",
       "  <line x1=\"207\" y1=\"25\" x2=\"207\" y2=\"145\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"120.96521372150383,25.965213721503837 207.96521372150383,25.965213721503837 207.96521372150383,145.96521372150383 120.96521372150383,145.96521372150383\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"164.465214\" y=\"165.965214\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >928</text>\n",
       "  <text x=\"227.965214\" y=\"85.965214\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,227.965214,85.965214)\">1280</text>\n",
       "  <text x=\"97.982607\" y=\"152.982607\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,97.982607,152.982607)\">464</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<from-zarr, shape=(1, 1, 464, 1280, 928), dtype=uint8, chunksize=(1, 1, 128, 128, 128), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.array as da\n",
    "print(output_seg_path)\n",
    "lazy_seg = da.from_zarr(output_seg_path)\n",
    "lazy_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab34631d-de74-4496-b317-cb9be425b5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lazy_seg.compute().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b64ca62-9400-43cc-aea3-f2dd2dffbe66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(lazy_seg[0, 0, 400, ...], vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca98266-3c79-47f3-87a1-aec8c6e0fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, sample in enumerate(zarr_data_loader):\n",
    "#     block = sample.batch_tensor[None, ...].to(cuda_device)\n",
    "#     pred_mask, prob_mask = segmentation_model.predict(\n",
    "#         batch=block,\n",
    "#         threshold=0.5,\n",
    "#     )\n",
    "\n",
    "#     (\n",
    "#         global_coord_pos,\n",
    "#         global_coord_positions_start,\n",
    "#         global_coord_positions_end,\n",
    "#     ) = recover_global_position(\n",
    "#         super_chunk_slice=sample.batch_super_chunk[0],\n",
    "#         internal_slices=sample.batch_internal_slice,\n",
    "#     )\n",
    "\n",
    "#     global_coord_positions_start = np.array(global_coord_positions_start[0])\n",
    "#     global_coord_positions_end = np.array(global_coord_positions_end[0])\n",
    "\n",
    "#     # start_condition = np.where(\n",
    "#     #     (global_coord_positions_start > 0) &\n",
    "#     #     (global_coord_positions_start != shape)\n",
    "#     # )[0]\n",
    "\n",
    "#     # global_coord_positions_start[start_condition] += (axis_pad * (2**len(start_condition)))\n",
    "\n",
    "#     # end_condition = np.where(\n",
    "#     #     (global_coord_positions_end > 0) &\n",
    "#     #     (global_coord_positions_end != shape)\n",
    "#     # )[0]\n",
    "\n",
    "#     # global_coord_positions_end[start_condition] += (axis_pad * (2**len(start_condition)))\n",
    "\n",
    "#     start_condition = np.where(\n",
    "#         (global_coord_positions_start == 0)\n",
    "#     )[0]\n",
    "\n",
    "#     if len(start_condition) == len(shape):\n",
    "#         new_global_stop = global_coord_positions_end\n",
    "\n",
    "#     else:\n",
    "        \n",
    "        \n",
    "\n",
    "#     new_global_coords = []\n",
    "#     for i in range(len(global_coord_positions_start)):\n",
    "#         new_global_coords.append(\n",
    "#             slice(\n",
    "#                 global_coord_positions_start[i],\n",
    "#                 global_coord_positions_end[i],\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     new_global_coords = (slice(0, 1), slice(0, 1), ) + tuple(new_global_coords)\n",
    "\n",
    "#     # output_intermediate_seg[new_global_coords] = pred_mask\n",
    "    \n",
    "#     # print(global_coord_positions_start, global_coord_positions_end, global_coord_pos, new_global_coords, shape)\n",
    "\n",
    "#     # Pinned?: {sample.batch_tensor.is_pinned()} - dtype: {sample.batch_tensor.dtype}\n",
    "    \n",
    "#     print(\n",
    "#         f\"Tensor shape: {sample.batch_tensor.shape} - Pred mask -> {pred_mask.shape} Global coords: {global_coord_pos} - new global {new_global_coords}\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f704c02f-ac2c-4628-8d15-5a3117e5d768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([120, 120])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = np.array([120, 120, 120])\n",
    "shape = np.array([512, 1408, 1024])\n",
    "\n",
    "condition = np.where( (position > 0) & (position != shape) )[0]\n",
    "position[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89c8fb3-340c-40b8-937f-1300c402d86a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60958caf-df1d-491b-84a6-cc80cc6b4e54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
